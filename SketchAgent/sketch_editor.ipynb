{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 245); perhaps you escaped the end quote? (2122788854.py, line 245)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 245\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"        system_message_json = [{\\\"role\\\": \\&q...\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 245); perhaps you escaped the end quote?\n"
     ]
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"75f9bbba-0a24-4703-9365-101dbbce35ea\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load the autoreload extension\\n\",\n",
    "    \"%load_ext autoreload\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set autoreload mode to 2\\n\",\n",
    "    \"%autoreload 2\\n\",\n",
    "    \"\\n\",\n",
    "    \"import base64\\n\",\n",
    "    \"from getpass import getpass\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"\\n\",\n",
    "    \"from io import BytesIO\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"from PIL import Image, ImageDraw, ImageFont\\n\",\n",
    "    \"from IPython.display import display, SVG\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from dotenv import load_dotenv\\n\",\n",
    "    \"import anthropic\\n\",\n",
    "    \"import torchvision.transforms as transforms\\n\",\n",
    "    \"import xml.etree.ElementTree as ET\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import ast\\n\",\n",
    "    \"import cairosvg\\n\",\n",
    "    \"\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append(\\\"..\\\") # Adds higher directory to python modules path.\\n\",\n",
    "    \"from prompts import sketch_first_prompt, idea_system_prompt, gt_example\\n\",\n",
    "    \"import utils\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"id\": \"45fcb49b-87c5-468c-ace9-8ff1c47951a1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"load_dotenv()\\n\",\n",
    "    \"claude_key = os.getenv(\\\"ANTHROPIC_API_KEY\\\")\\n\",\n",
    "    \"client = anthropic.Anthropic(api_key=claude_key)\\n\",\n",
    "    \"model = \\\"claude-3-5-sonnet-20240620\\\"\\n\",\n",
    "    \"gen_mode = \\\"generation\\\"\\n\",\n",
    "    \"max_tokens=3000\\n\",\n",
    "    \"\\n\",\n",
    "    \"res=50\\n\",\n",
    "    \"init_canvas, cells_to_pixels_map = utils.create_grid_image(res=res, cell_size=12, header_size=12)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"578fe0a2-b2db-4eac-8fb3-4d8e78b9afc7\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Utils\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"09ae2c08-e378-40dc-8428-89a1be3d0f5f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def load_sketch_data(path_to_data, object_to_edit, cache=False):\\n\",\n",
    "    \"    path_to_sketch_im = f\\\"{path_to_data}/{object_to_edit}/output_{object_to_edit}_canvas.png\\\"\\n\",\n",
    "    \"    path_to_json = f\\\"{path_to_data}/{object_to_edit}/experiment_log.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    with open(path_to_json, 'r') as file:\\n\",\n",
    "    \"        experiment_log = json.load(file)\\n\",\n",
    "    \"        if cache:\\n\",\n",
    "    \"            system_prompt = experiment_log[0][\\\"content\\\"][0][\\\"text\\\"]\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            system_prompt = experiment_log[0][\\\"content\\\"]\\n\",\n",
    "    \"        assitant_prompt = experiment_log[-1]['content'][0]['text']\\n\",\n",
    "    \"        msg_history = experiment_log[1:]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    sketch_rendered = Image.open(path_to_sketch_im)\\n\",\n",
    "    \"    return sketch_rendered, system_prompt, msg_history, assitant_prompt\\n\",\n",
    "    \"\\n\",\n",
    "    \"def call_llm(system_message, other_msg, cache, additional_args):\\n\",\n",
    "    \"    if cache:\\n\",\n",
    "    \"        init_response = client.beta.prompt_caching.messages.create(\\n\",\n",
    "    \"                model=model,\\n\",\n",
    "    \"                max_tokens=max_tokens,\\n\",\n",
    "    \"                system=system_message,\\n\",\n",
    "    \"                messages=other_msg,\\n\",\n",
    "    \"                **additional_args\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        init_response = client.messages.create(\\n\",\n",
    "    \"                model=model,\\n\",\n",
    "    \"                max_tokens=max_tokens,\\n\",\n",
    "    \"                system=system_message,\\n\",\n",
    "    \"                messages=other_msg,\\n\",\n",
    "    \"                **additional_args\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"    return init_response\\n\",\n",
    "    \"    \\n\",\n",
    "    \"def define_input_to_llm(msg_history, init_canvas_str, msg, cache):\\n\",\n",
    "    \"    # other_msg should contain all messgae without the system prompt\\n\",\n",
    "    \"    other_msg = msg_history \\n\",\n",
    "    \"\\n\",\n",
    "    \"    content = []\\n\",\n",
    "    \"    # Claude best practice is image-then-text\\n\",\n",
    "    \"    if init_canvas_str is not None:\\n\",\n",
    "    \"        content.append({\\\"type\\\": \\\"image\\\", \\\"source\\\": {\\\"type\\\": \\\"base64\\\", \\\"media_type\\\": \\\"image/jpeg\\\", \\\"data\\\": init_canvas_str}}) \\n\",\n",
    "    \"\\n\",\n",
    "    \"    content.append({\\\"type\\\": \\\"text\\\", \\\"text\\\": msg})\\n\",\n",
    "    \"    if cache:\\n\",\n",
    "    \"        content[-1][\\\"cache_control\\\"] = {\\\"type\\\": \\\"ephemeral\\\"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"    other_msg = other_msg + [{\\\"role\\\": \\\"user\\\", \\\"content\\\": content}]\\n\",\n",
    "    \"    return other_msg\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_response_from_llm(\\n\",\n",
    "    \"        msg,\\n\",\n",
    "    \"        system_message,\\n\",\n",
    "    \"        msg_history=[],\\n\",\n",
    "    \"        init_canvas_str=None,\\n\",\n",
    "    \"        prefill_msg=None,\\n\",\n",
    "    \"        seed_mode=\\\"stochastic\\\",\\n\",\n",
    "    \"        stop_sequences=None,\\n\",\n",
    "    \"        gen_mode=\\\"generation\\\",\\n\",\n",
    "    \"        cache=True,\\n\",\n",
    "    \"        path2save=None\\n\",\n",
    "    \"    ):  \\n\",\n",
    "    \"        additional_args = {}\\n\",\n",
    "    \"        if seed_mode == \\\"deterministic\\\":\\n\",\n",
    "    \"            additional_args[\\\"temperature\\\"] = 0.0\\n\",\n",
    "    \"            additional_args[\\\"top_k\\\"] = 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if cache:\\n\",\n",
    "    \"            system_message = [{\\n\",\n",
    "    \"                \\\"type\\\": \\\"text\\\",\\n\",\n",
    "    \"                \\\"text\\\": system_message,\\n\",\n",
    "    \"                \\\"cache_control\\\": {\\\"type\\\": \\\"ephemeral\\\"}\\n\",\n",
    "    \"            }]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # other_msg should contain all messgae without the system prompt\\n\",\n",
    "    \"        other_msg = define_input_to_llm(msg_history, init_canvas_str, msg, cache) \\n\",\n",
    "    \"\\n\",\n",
    "    \"        if gen_mode == \\\"completion\\\":\\n\",\n",
    "    \"            if prefill_msg:\\n\",\n",
    "    \"                other_msg = other_msg + [{\\\"role\\\": \\\"assistant\\\", \\\"content\\\": f\\\"{prefill_msg}\\\"}]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # in case of stroke by stroke generation\\n\",\n",
    "    \"        if stop_sequences:\\n\",\n",
    "    \"            additional_args[\\\"stop_sequences\\\"]= [stop_sequences]\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            additional_args[\\\"stop_sequences\\\"]= [\\\"</answer>\\\"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Note that we deterministic settings for reproducibility (temperature=0.0 and top_k=1). \\n\",\n",
    "    \"        # To run in stochastic mode just comment these parameters.\\n\",\n",
    "    \"        response = call_llm(system_message, other_msg, cache, additional_args)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        content = response.content[0].text\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if gen_mode == \\\"completion\\\":\\n\",\n",
    "    \"            other_msg = other_msg[:-1] # remove initial assistant prompt\\n\",\n",
    "    \"            content = f\\\"{prefill_msg}{content}\\\" \\n\",\n",
    "    \"\\n\",\n",
    "    \"        # saves to json\\n\",\n",
    "    \"        if path2save is not None:\\n\",\n",
    "    \"            system_message_json = [{\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message}]\\n\",\n",
    "    \"            new_msg_history = other_msg + [\\n\",\n",
    "    \"                {\\n\",\n",
    "    \"                    \\\"role\\\": \\\"assistant\\\",\\n\",\n",
    "    \"                    \\\"content\\\": [\\n\",\n",
    "    \"                        {\\n\",\n",
    "    \"                            \\\"type\\\": \\\"text\\\",\\n\",\n",
    "    \"                            \\\"text\\\": content,\\n\",\n",
    "    \"                        }\\n\",\n",
    "    \"                    ],\\n\",\n",
    "    \"                }\\n\",\n",
    "    \"            ]    \\n\",\n",
    "    \"            with open(f\\\"{path2save}/experiment_log.json\\\", 'w') as json_file:\\n\",\n",
    "    \"                json.dump(system_message_json + new_msg_history, json_file, indent=4)\\n\",\n",
    "    \"            print(f\\\"Data has been saved to [{path2save}/experiment_log.json]\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        return content, new_msg_history\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def save_sketch(model_strokes_svg, output_path, add_object, init_canvas):\\n\",\n",
    "    \"    with open(f\\\"{output_path}/output_{add_object}.svg\\\", \\\"w\\\") as svg_file:\\n\",\n",
    "    \"        svg_file.write(model_strokes_svg)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    # save the result also without the canvas background\\n\",\n",
    "    \"    cairosvg.svg2png(url=f\\\"{output_path}/output_{add_object}.svg\\\", write_to=f\\\"{output_path}/output_{add_object}.png\\\", background_color=\\\"white\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if init_canvas is not None:\\n\",\n",
    "    \"        # save the result as png on the canvas background \\n\",\n",
    "    \"        output_png_path = f\\\"{output_path}/output_{add_object}_canvas.png\\\"\\n\",\n",
    "    \"        cairosvg.svg2png(url=f\\\"{output_path}/output_{add_object}.svg\\\", write_to=output_png_path)\\n\",\n",
    "    \"        foreground = Image.open(output_png_path)\\n\",\n",
    "    \"        init_canvas_copy = init_canvas.copy()\\n\",\n",
    "    \"        init_canvas_copy.paste(Image.open(output_png_path), (0, 0), foreground) \\n\",\n",
    "    \"        init_canvas_copy.save(output_png_path)\\n\",\n",
    "    \"        return init_canvas_copy\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"3db0a1f2-0e1c-43fb-a611-690fe9100f70\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Edit\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e83e40ec-30da-451a-a1bf-5e6ad13605b8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Edit mode - Add\\n\",\n",
    "    \"Adds content to existing sketch (note the prompt are adjusted to the type of edit)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"id\": \"da19d0f1-ae00-41a7-a072-f4e275b676f3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def edit_sketch_in_chat_add(path_to_data, object_to_edit, add_objects, reflection_prompt, cache, seed_mode=\\\"deterministic\\\"):\\n\",\n",
    "    \"    output_path = f\\\"{path_to_data}/{object_to_edit}/editing_add\\\"\\n\",\n",
    "    \"    if not os.path.exists(output_path):\\n\",\n",
    "    \"        os.makedirs(output_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Load sketch data\\n\",\n",
    "    \"    sketch_rendered, system_prompt, msg_history, assitant_prompt = load_sketch_data(path_to_data, object_to_edit, cache)\\n\",\n",
    "    \"    with open(f\\\"{output_path}/experiment_log.json\\\", 'w') as json_file:\\n\",\n",
    "    \"        system_message_json = [{\\\"role\\\": \\&q..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
